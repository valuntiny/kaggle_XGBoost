{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guojing Wu** *| 2019-07-21*\n",
    "\n",
    "<a href = \"https://www.kaggle.com/alexisbcook/xgboost\"> Kaggle: XGBoost </a>\n",
    "\n",
    "# XGBoost - Decision Tree Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter tuning\n",
    "\n",
    "1) n_estimators, also know as the number of models.\n",
    "\n",
    "2) early_stopping_rounds, early stop cause the model to stop running when validation score doesn't improve anymore. But since there are random chances when validation score doesn't improve, this para specify how many rounds of deterioration are allowed before stop. Come along with `eval_set` para.\n",
    "\n",
    "3) learning_rate, multiple the prediciton of each model by a small number before adding them in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in general, a small learning rate and big n_estimators would be good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# read data\n",
    "X = pd.read_csv(\"train.csv\", index_col='Id')\n",
    "X_test_full = pd.read_csv(\"test.csv\", index_col='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split as X and y\n",
    "X.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X.SalePrice\n",
    "X.drop(axis=0, columns=['SalePrice'], inplace=True)\n",
    "X_train_full, X_val_full, y_train, y_val = train_test_split(X, y, train_size=0.8, test_size=0.2, \n",
    "                                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only choose the numercial column and low cardinality categorical column\n",
    "low_cardinality_col = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10\n",
    "                      and X_train_full[cname].dtype == 'object']\n",
    "numerical_col = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "my_col = low_cardinality_col + numerical_col\n",
    "\n",
    "# make a harder copy\n",
    "X_train = X_train_full[my_col].copy()\n",
    "X_val = X_val_full[my_col].copy()\n",
    "X_test = X_test_full[my_col].copy()\n",
    "\n",
    "# one-hot encode, and also use align() to make sure only the dummies in train exist\n",
    "X_train = pd.get_dummies(X_train)\n",
    "X_val = pd.get_dummies(X_val)\n",
    "X_test = pd.get_dummies(X_test)\n",
    "X_train, X_val = X_train.align(X_val, axis=1, join='left')\n",
    "X_train, X_test = X_train.align(X_test, axis=1, join='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start XBGoost\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_1 = XGBRegressor(random_state=0)\n",
    "my_model_1.fit(X_train, y_train)\n",
    "prediction_1 = my_model_1.predict(X_val)\n",
    "mae_1 = mean_absolute_error(prediction_1, y_val)\n",
    "print(\"MAE for model 1:\", mae_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve the model\n",
    "my_model_2 = XGBRegressor(random_state=0, n_estimators=1000, learning_rate=0.1)\n",
    "my_model_2.fit(X_train, y_train)\n",
    "prediction_2 = my_model_2.predict(X_val)\n",
    "mae_2 = mean_absolute_error(prediction_2, y_val)\n",
    "print(\"MAE for model 2:\", mae_2)\n",
    "\n",
    "# turns out it did improve a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give a bad model\n",
    "my_model_3 = XGBRegressor(random_state=0, n_estimators=1)\n",
    "my_model_3.fit(X_train, y_train)\n",
    "prediction_3 = my_model_3.predict(X_val)\n",
    "mae_3 = mean_absolute_error(prediction_3, y_val)\n",
    "print(\"MAE for model 3:\", mae_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
